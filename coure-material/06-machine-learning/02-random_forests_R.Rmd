---
title: "ML Algorithms for classification"
author: "Hicham Zmarrou"
date: "Notebook -- <http://bit.ly/2q9NPSU>  <br /> <br />"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    highlight: pygments
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
subtitle: Decision trees
venue: GL R courses
---


<hr>

[Visit my website](http://trefoil.ml/) for more like this!

__References__

Most of this material is borrowed from:

* Textbook: [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)

____________________________________
Let us read in the data and explore it. We can read in the data directly from the page using the read.table function.

```{r}
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
wine <- read.csv(url, sep = ";")
head(wine)
```

Let us look at the distribution of the wine quality. We can use barplot for this.

```{r}
barplot(table(wine$quality))
```

As we can see, there are a lot of wines with a quality of 6 as compared to the others. The dataset description states - there are a lot more normal wines than excellent or poor ones. For the purpose of this discussion, let's classify the wines into good, bad, and normal based on their quality.

```{r}
wine$taste <- ifelse(wine$quality < 6, 'bad', 'good')
wine$taste[wine$quality == 6] <- 'normal'
wine$taste <- as.factor(wine$taste)
```



This will classify all wines into bad, normal, or good, depending on whether their quality is less than, equal to, or greater than 6 respectively. Let's look at the distribution again.
```{r}
table(wine$taste)
 

```
Before we build our model, let's separate our data into testing and training sets.


```{r}

set.seed(123)
samp <- sample(nrow(wine), 0.6 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]

```

This will place 60% of the observations in the original dataset into train and the remaining 40% of the observations into test.

## Building the model
Now, we are ready to build our model. We will need the randomForest library for this.


```{r}
library(randomForest)
model <- randomForest(taste ~ . - quality, data = train)

model
```


We can use ntree and mtry to specify the total number of trees to build (default = 500), and the number of predictors to randomly sample at each split respectively. Let's take a look at the model.


We can see that 500 trees were built, and the model randomly sampled 3 predictors at each split. It also shows a matrix containing prediction vs actual, as well as classification error for each class. Let's test the model on the test data set.


```{r}
pred <- predict(model, newdata = test)
table(pred, test$taste)
```
We can test the accuracy as follows:
```{r}
(482 + 252 + 667) / nrow(test)
```

There we have it! We achieved ~71.5% accuracy with a very simple model. It could be further improved by feature selection, and possibly by trying different values of mtry.

That brings us to the end of the article. I hope you enjoyed it! As always, if you have questions or feedback, feel free to reach out to me on Twitter or leave a comment below!

###############
###Exercies#### 
###############
 type help(RandomForest) and read the description of the different parameters of this function

